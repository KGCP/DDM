# Arxiv Experiment

This repository contains the experiment for our Arxiv research project titled "ASKG: An Approach to Enrich Scholarly Knowledge Graphs"

# Simple Chunking - Benchmark Experiment
1.Data Preprocessing:
Dataset composition: Ten scientific articles were selected as the dataset.
Chunking: A simple chunking approach was employed, with a maximum of 100 tokens per chunk and a 5% overlap ratio.

2.Embedding Vector Generation:
The DistilBERT model was used to generate embedding vectors for all text chunks and user queries.

3.Similarity Calculation and Selection:
The cosine similarity between each text chunk and the query was computed. The top N most similar chunks (N = 1, 3, 5, 10, 15) were selected for different experimental conditions.

4.Query Answering:
The selected text chunks were used as context, and the Llama2 model was employed to answer user queries solely based on the selected context, without using any external data.

# Knowledge Graph Approach
1.Knowledge Graph Construction:

The PARSE+DDM method was used to create a knowledge graph (.ttl file) from the ten scientific articles, containing information derived from the decomposed articles.
Embedding Vector Generation:

The GIST Embedding model was utilized to associate the attributes (such as paragraphs and summaries) in the knowledge graph with the paragraphs of the scientific articles.

2.Entity Recognition and Matching:

A language model (LLM) was employed to identify entities in user queries, and these entities were matched with the academic entities in the knowledge graph to retrieve relevant paragraph and summary information.

3.Paragraph Selection:

The Keyword Frequency Matching method was used to select the top 10 paragraphs most relevant to the query.
Among the top 10 paragraphs, the Maximal Marginal Relevance (MMR) method was applied to select the top 5 most diverse paragraphs.

4.Query Answering:

The selected 5 diverse paragraphs were used as context and fed into the Llama2 model to generate answers to user queries.

5.KG Experiments:

The structured information from the knowledge graph was utilized to further optimize and supplement the query answers. The following directions were considered:

a. Entity Linking: Linking the entities in the answers with the entities in the knowledge graph to provide richer entity information.

b. Relation Reasoning: Leveraging the relation information in the knowledge graph to perform reasoning and expand the answers.

c. Knowledge Fusion: Integrating the knowledge from the knowledge graph with the generated answers to improve the accuracy and completeness of the answers.

6.Evaluation Methods:

Embedding Distance Evaluation: The GIST-Embedding was used to compare the paragraphs obtained in the Benchmark and KG experiments, calculating the semantic distance between them. A smaller distance indicates that the paragraphs selected by the KG experiment are more similar to the results of the Benchmark experiment.
Human Evaluation: Experts or users were invited to score the answers generated by the two experiments, evaluating the relevance, accuracy, and completeness of the answers. A scoring standard, such as a 5-point scale, was designed to ensure consistency in the evaluation.

7.Experimental Result Analysis:

The evaluation results of the Benchmark and KG experiments were compared to analyze the impact of introducing the knowledge graph on the QA performance.
The effectiveness of the KG-based extension steps in the KG experiment was analyzed, exploring the role and value of knowledge graphs in QA tasks.
The advantages, limitations, and future improvement directions of the experiments were summarized.
